{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anomalous sound detection (ASD) with Kubeflow Pipeline**\n",
    "Anomalous sound detection (ASD) is the task of identifying whether the sound emitted from a machine is normal or anomalous. Automatic detection of mechanical failure is essential technology in the fourth industrial revolution, including artificial intelligence (AI)-based factory automation. Prompt detection of machine anomalies by observing sounds is useful for machine condition monitoring.\n",
    "\n",
    "In this notebook, we try to adapt the code provided by the github project [dcase2020_task2_baseline](https://github.com/y-kawagu/dcase2020_task2_baseline) under the [MIT License](https://github.com/y-kawagu/dcase2020_task2_baseline/blob/master/LICENSE) whose main goals are \"Unsupervised Detection of Anomalous Sounds for Machine Condition Monitoring\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 Install packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip requirements\n",
    "!python -m pip install --user --upgrade pip\n",
    "!python -m pip install --user urllib3\n",
    "!pip install --user --upgrade kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the installation, we need to restart kernel for changes to take effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 Import libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.components import OutputPath, InputPath, func_to_container_op, OutputBinaryFile, InputBinaryFile\n",
    "import kfp.components as comp\n",
    "import typing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 Download dataset**\n",
    "\n",
    "One pipeline parameter will be the url of the dataset we are going to focus. That url should lead to a dataset that has the following structure:\n",
    "\n",
    "\n",
    "/machine( Toy_Car / fan / slider ...)\n",
    "\n",
    "    /train (Only normal data for all Machine IDs are included.)\n",
    "        /normal_id_01_00000000.wav\n",
    "        ...\n",
    "        /normal_id_01_00000999.wav\n",
    "        /normal_id_02_00000000.wav\n",
    "        ...\n",
    "        /normal_id_04_00000999.wav\n",
    "    /test (Normal and anomaly data for all Machine IDs are included.)\n",
    "        /normal_id_01_00000000.wav\n",
    "        ...\n",
    "        /normal_id_01_00000349.wav\n",
    "        /anomaly_id_01_00000000.wav\n",
    "        ...\n",
    "        /anomaly_id_01_00000263.wav\n",
    "        /normal_id_02_00000000.wav\n",
    "        ...\n",
    "        /anomaly_id_04_00000264.wav\n",
    "        \n",
    "Some options for the dataset url are:\n",
    "\n",
    "* Fan dataset: https://zenodo.org/record/3678171/files/dev_data_fan.zip\n",
    "* Toy-car dataset:https://zenodo.org/record/3678171/files/dev_data_ToyCar.zip\n",
    "* Slider rail dataset: https://zenodo.org/record/3678171/files/dev_data_slider.zip\n",
    "* Toy-conveyor dataset: https://zenodo.org/record/3678171/files/dev_data_ToyConveyor.zip\n",
    "* Valve dataset: https://zenodo.org/record/3678171/files/dev_data_valve.zip\n",
    "* Pump dataset: https://zenodo.org/record/3678171/files/dev_data_pump.zip \n",
    "\n",
    "The following python function will become a component later. This component will download the dataset in a shared space between pods, so it can be accessed by the others components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(dataset_url, dataset_path: comp.OutputPath(str)):\n",
    "    import requests\n",
    "    import zipfile\n",
    "    import tempfile\n",
    "    import os\n",
    "\n",
    "    os.makedirs(dataset_path + '/data', exist_ok=True)\n",
    "    r = requests.get(dataset_url, stream=True)\n",
    "    with tempfile.TemporaryFile() as tf:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            tf.write(chunk)\n",
    "        with zipfile.ZipFile(tf, \"r\") as f:\n",
    "            f.extractall(dataset_path + '/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4 Training component**\n",
    "\n",
    "The following python function will become a component later. This component gets the reference from the dataset and another pipeline parameters. \n",
    "Using all the parameters, trains a model and saves it in a temporal shared directory. It also generates a loss plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset_path: comp.InputPath(str), feature_frames,\n",
    "          feature_hop_length, feature_n_fft, feature_n_mels, feature_power, fit_batch_size, fit_compile_loss,\n",
    "          fit_compile_optimizer, fit_epochs, fit_shuffle, fit_validation_split, fit_verbose, max_fpr, lossplot_path: comp.OutputPath(str),\n",
    "          models_dir: comp.OutputPath()) -> typing.NamedTuple('loss_plot', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    import base64\n",
    "    import matplotlib.pyplot as plt\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    import sys\n",
    "    import os\n",
    "    import glob\n",
    "    import numpy\n",
    "    import librosa\n",
    "    import librosa.core\n",
    "    import librosa.feature\n",
    "\n",
    "    # Parse arguments from pipeline\n",
    "    feature_frames = int(feature_frames)\n",
    "    feature_hop_length = int(feature_hop_length)\n",
    "    feature_n_fft = int(feature_n_fft)\n",
    "    feature_n_mels = int(feature_n_mels)\n",
    "    feature_power = float(feature_power)\n",
    "    fit_batch_size = int(fit_batch_size)\n",
    "    fit_epochs = int(fit_epochs)\n",
    "    fit_validation_split = float(fit_validation_split)\n",
    "    fit_verbose = int(fit_verbose)\n",
    "    max_fpr = float(max_fpr)\n",
    "\n",
    "    def select_dirs(dataset_path):\n",
    "        \"\"\"\n",
    "        return :\n",
    "                dirs :  list [ str ]\n",
    "                    load base directory list of data\n",
    "        \"\"\"\n",
    "        print(\"load_directory <- data\")\n",
    "        dir_path = os.path.abspath(dataset_path + \"{base}/*\".format(base='/data'))\n",
    "        dirs = sorted(glob.glob(dir_path))\n",
    "        return dirs\n",
    "\n",
    "    def file_to_vector_array(file_name, n_mels=64, frames=5, n_fft=1024, hop_length=512, power=2.0):\n",
    "        \"\"\"\n",
    "        convert file_name to a vector array.\n",
    "\n",
    "        file_name : str\n",
    "            target .wav file\n",
    "\n",
    "        return : numpy.array( numpy.array( float ) )\n",
    "            vector array\n",
    "            * dataset.shape = (dataset_size, feature_vector_length)\n",
    "        \"\"\"\n",
    "        dims = n_mels * frames\n",
    "        y, sr = file_load(file_name)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
    "        log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
    "        vector_array_size = len(log_mel_spectrogram[0, :]) - frames + 1\n",
    "        if vector_array_size < 1:\n",
    "            return numpy.empty((0, dims))\n",
    "        vector_array = numpy.zeros((vector_array_size, dims))\n",
    "        for t in range(frames):\n",
    "            vector_array[:, n_mels * t: n_mels * (t + 1)] = log_mel_spectrogram[:, t: t + vector_array_size].T\n",
    "        return vector_array\n",
    "\n",
    "    def file_load(wav_name, mono=False):\n",
    "        \"\"\"\n",
    "        load .wav file.\n",
    "        wav_name : str\n",
    "            target .wav file\n",
    "        sampling_rate : int\n",
    "            audio file sampling_rate\n",
    "        mono : boolean\n",
    "            When load a multi channels file and this param True, the returned data will be merged for mono data\n",
    "        return : numpy.array( float )\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            return librosa.load(wav_name, sr=None, mono=mono)\n",
    "        except Exception:\n",
    "            print(\"Error: file_broken or not exists!! : {}\".format(wav_name))\n",
    "\n",
    "    def file_list_generator(target_dir, dir_name=\"train\", ext=\"wav\"):\n",
    "        \"\"\"\n",
    "        target_dir : str\n",
    "            base directory path of the data\n",
    "        dir_name : str (default=\"train\")\n",
    "            directory name containing training data\n",
    "        ext : str (default=\"wav\")\n",
    "            file extension of audio files\n",
    "        return :\n",
    "            train_files : list [ str ]\n",
    "                file list for training\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"target_dir : {}\".format(target_dir))\n",
    "        training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
    "        files = sorted(glob.glob(training_list_path))\n",
    "        if len(files) == 0:\n",
    "            print(\"Exception: no_wav_file!!\")\n",
    "\n",
    "        print(\"train_file num : {num}\".format(num=len(files)))\n",
    "        return files\n",
    "\n",
    "    def list_to_vector_array(file_list, msg=\"calc...\", n_mels=64, frames=5, n_fft=1024, hop_length=512, power=2.0):\n",
    "        \"\"\"\n",
    "        convert the file_list to a vector array.\n",
    "        file_to_vector_array() is iterated, and the output vector array is concatenated.\n",
    "        file_list : list [ str ]\n",
    "            .wav filename list of dataset\n",
    "        msg : str ( default = \"calc...\" )\n",
    "            description for tqdm.\n",
    "            this parameter will be input into \"desc\" param at tqdm.\n",
    "        return : numpy.array( numpy.array( float ) )\n",
    "            vector array for training (this function is not used for test.)\n",
    "            * dataset.shape = (number of feature vectors, dimensions of feature vectors)\n",
    "        \"\"\"\n",
    "        dims = n_mels * frames\n",
    "        for idx in range(len(file_list)):\n",
    "            vector_array = file_to_vector_array(file_list[idx], n_mels=n_mels, frames=frames, n_fft=n_fft, hop_length=hop_length, power=power)\n",
    "            if idx == 0:\n",
    "                dataset = numpy.zeros((vector_array.shape[0] * len(file_list), dims), float)\n",
    "            dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n",
    "        return dataset\n",
    "\n",
    "    def get_model(inputDim):\n",
    "        from tensorflow.keras.models import Model\n",
    "        from tensorflow.keras.layers import Input, Dense, BatchNormalization, Activation\n",
    "        \"\"\"\n",
    "        define the keras model\n",
    "        the model based on the simple dense auto encoder\n",
    "        (128*128*128*128*8*128*128*128*128)\n",
    "        \"\"\"\n",
    "        inputLayer = Input(shape=(inputDim,))\n",
    "\n",
    "        h = Dense(128)(inputLayer)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation('relu')(h)\n",
    "\n",
    "        h = Dense(128)(h)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation('relu')(h)\n",
    "\n",
    "        h = Dense(128)(h)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation('relu')(h)\n",
    "\n",
    "        h = Dense(128)(h)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation('relu')(h)\n",
    "\n",
    "        h = Dense(8)(h)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation('relu')(h)\n",
    "\n",
    "        h = Dense(128)(h)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation('relu')(h)\n",
    "\n",
    "        h = Dense(128)(h)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation('relu')(h)\n",
    "\n",
    "        h = Dense(128)(h)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation('relu')(h)\n",
    "\n",
    "        h = Dense(128)(h)\n",
    "        h = BatchNormalization()(h)\n",
    "        h = Activation('relu')(h)\n",
    "\n",
    "        h = Dense(inputDim)(h)\n",
    "\n",
    "        return Model(inputs=inputLayer, outputs=h)\n",
    "\n",
    "    def save_loss_plot(history, plot_path):\n",
    "        \"\"\"\n",
    "        history: History object from keras. Its History.history attribute is a record of training loss and validation loss values.\n",
    "        plot_path: path where plot image will be saved.\n",
    "        \"\"\"\n",
    "        # Creation of the plot\n",
    "        loss, val_loss = history.history[\"loss\"], history.history[\"val_loss\"]\n",
    "        fig = plt.figure(figsize=(30, 10))\n",
    "        plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        ax.cla()\n",
    "        ax.plot(loss)\n",
    "        ax.plot(val_loss)\n",
    "        ax.set_title(\"Model loss\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        ax.legend([\"Train\", \"Validation\"], loc=\"upper right\")\n",
    "\n",
    "        # Saving plot in specified path\n",
    "        with open(plot_path, \"wb\") as fd:\n",
    "            plt.savefig(fd)\n",
    "\n",
    "    def get_web_app_from_loss_plot(plot_path):\n",
    "        \"\"\"\n",
    "        plot_path: path where plot image is saved.\n",
    "        return: JSON object representing kubeflow output viewer for web-app.\n",
    "        \"\"\"\n",
    "        # Retrieve encoded bytes of the specified image path\n",
    "        encoded = base64.b64encode(open(plot_path, \"rb\").read()).decode('latin1')\n",
    "\n",
    "        web_app_json = {\n",
    "            'type': 'web-app',\n",
    "            'storage': 'inline',\n",
    "            'source': f\"\"\"<img width=\"100%\" src=\"data:image/png;base64,{encoded}\"/>\"\"\"\n",
    "        }\n",
    "        return web_app_json\n",
    "\n",
    "    dirs = select_dirs(dataset_path)\n",
    "\n",
    "    # loop of the base directory\n",
    "    for idx, target_dir in enumerate(dirs):\n",
    "        print(\"\\n===========================\")\n",
    "        print(\"[{idx}/{total}] {dirname}\".format(dirname=target_dir, idx=idx + 1, total=len(dirs)))\n",
    "        machine_type = os.path.split(target_dir)[1]\n",
    "        model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=models_dir + '/model', machine_type=machine_type)\n",
    "        if not os.path.exists(models_dir + '/model'):\n",
    "            os.makedirs(models_dir + '/model')\n",
    "\n",
    "        # generate dataset\n",
    "        print(\"============== DATASET_GENERATOR ==============\")\n",
    "        files = file_list_generator(target_dir)\n",
    "        train_data = list_to_vector_array(files,\n",
    "                                          msg=\"generate train_dataset\",\n",
    "                                          n_mels=feature_n_mels,\n",
    "                                          frames=feature_frames,\n",
    "                                          n_fft=feature_n_fft,\n",
    "                                          hop_length=feature_hop_length,\n",
    "                                          power=feature_power)\n",
    "        print(train_data)\n",
    "        # train model\n",
    "        print(\"============== MODEL TRAINING ==============\")\n",
    "        model = get_model(feature_n_mels * feature_frames)\n",
    "        model.summary()\n",
    "        model.compile(optimizer=fit_compile_optimizer, loss=fit_compile_loss,)\n",
    "        history = model.fit(train_data,\n",
    "                            train_data,\n",
    "                            epochs=fit_epochs,\n",
    "                            batch_size=fit_batch_size,\n",
    "                            shuffle=fit_shuffle,\n",
    "                            validation_split=fit_validation_split,\n",
    "                            verbose=fit_verbose)\n",
    "\n",
    "        model.save(model_file_path)\n",
    "\n",
    "    save_loss_plot(history, lossplot_path)\n",
    "    loss_plot = [get_web_app_from_loss_plot(lossplot_path)]\n",
    "\n",
    "    print(\"============== END TRAINING ==============\")\n",
    "\n",
    "    metadata = {\n",
    "        'outputs' : loss_plot\n",
    "    }\n",
    "    loss_plot = namedtuple('loss_plot', ['mlpipeline_ui_metadata'])\n",
    "    return loss_plot(json.dumps(metadata))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5 Testing component**\n",
    "\n",
    "The following python function will become a component later. This component gets the reference from the dataset, the reference from the model and another pipeline parameters. \n",
    "Using all the parameters, tests the model and saves score results temporal shared directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataset_path: comp.InputPath(str), feature_frames, feature_hop_length, feature_n_fft,\n",
    "         feature_n_mels, feature_power, fit_batch_size, fit_compile_loss, fit_compile_optimizer, fit_epochs, fit_shuffle,\n",
    "         fit_validation_split, fit_verbose, max_fpr, models_dir: comp.InputPath(),\n",
    "         anomaly_dir: comp.OutputPath(str), results_dir: comp.OutputPath(str), mlpipelinemetrics_path: comp.OutputPath(), labels_dir: comp.OutputPath()):\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    import csv\n",
    "    import re\n",
    "    import itertools\n",
    "    import sys\n",
    "    import numpy\n",
    "    from sklearn import metrics\n",
    "    import json\n",
    "    import tensorflow as tf\n",
    "    import librosa\n",
    "    import librosa.core\n",
    "    import librosa.feature\n",
    "\n",
    "    # Parse pipeline parameters\n",
    "    feature_frames = int(feature_frames)\n",
    "    feature_hop_length = int(feature_hop_length)\n",
    "    feature_n_fft = int(feature_n_fft)\n",
    "    feature_n_mels = int(feature_n_mels)\n",
    "    feature_power = float(feature_power)\n",
    "    fit_batch_size = int(fit_batch_size)\n",
    "    fit_epochs = int(fit_epochs)\n",
    "    fit_validation_split = float(fit_validation_split)\n",
    "    fit_verbose = int(fit_verbose)\n",
    "    max_fpr = float(max_fpr)\n",
    "\n",
    "    def select_dirs(dataset_path):\n",
    "        \"\"\"\n",
    "        return :\n",
    "                dirs :  list [ str ]\n",
    "                    load base directory list of data\n",
    "        \"\"\"\n",
    "        print(\"load_directory <- data\")\n",
    "        dir_path = os.path.abspath(dataset_path + \"{base}/*\".format(base='/data'))\n",
    "        dirs = sorted(glob.glob(dir_path))\n",
    "        return dirs\n",
    "\n",
    "    def file_to_vector_array(file_name, n_mels=64, frames=5, n_fft=1024, hop_length=512, power=2.0):\n",
    "        \"\"\"\n",
    "        convert file_name to a vector array.\n",
    "\n",
    "        file_name : str\n",
    "            target .wav file\n",
    "\n",
    "        return : numpy.array( numpy.array( float ) )\n",
    "            vector array\n",
    "            * dataset.shape = (dataset_size, feature_vector_length)\n",
    "        \"\"\"\n",
    "        dims = n_mels * frames\n",
    "        y, sr = file_load(file_name)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, power=power)\n",
    "        log_mel_spectrogram = 20.0 / power * numpy.log10(mel_spectrogram + sys.float_info.epsilon)\n",
    "        vector_array_size = len(log_mel_spectrogram[0, :]) - frames + 1\n",
    "        if vector_array_size < 1:\n",
    "            return numpy.empty((0, dims))\n",
    "        vector_array = numpy.zeros((vector_array_size, dims))\n",
    "        for t in range(frames):\n",
    "            vector_array[:, n_mels * t: n_mels * (t + 1)] = log_mel_spectrogram[:, t: t + vector_array_size].T\n",
    "        return vector_array\n",
    "\n",
    "    def file_load(wav_name, mono=False):\n",
    "        \"\"\"\n",
    "        load .wav file.\n",
    "        wav_name : str\n",
    "            target .wav file\n",
    "        sampling_rate : int\n",
    "            audio file sampling_rate\n",
    "        mono : boolean\n",
    "            When load a multi channels file and this param True, the returned data will be merged for mono data\n",
    "        return : numpy.array( float )\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            return librosa.load(wav_name, sr=None, mono=mono)\n",
    "        except Exception:\n",
    "            print(\"Error: file_broken or not exists!! : {}\".format(wav_name))\n",
    "\n",
    "    def load_model(file_path):\n",
    "        \"\"\"\n",
    "        return:\n",
    "            model loaded from file_path\n",
    "        \"\"\"\n",
    "        return tf.keras.models.load_model(file_path)\n",
    "\n",
    "    def get_machine_id_list_for_test(target_dir, dir_name=\"test\", ext=\"wav\"):\n",
    "        \"\"\"\n",
    "        target_dir : str\n",
    "            base directory path of \"dev_data\" or \"eval_data\"\n",
    "        test_dir_name : str (default=\"test\")\n",
    "            directory containing test data\n",
    "        ext : str (default=\"wav)\n",
    "            file extension of audio files\n",
    "\n",
    "        return :\n",
    "            machine_id_list : list [ str ]\n",
    "                list of machine IDs extracted from the names of test files\n",
    "        \"\"\"\n",
    "        # create test files\n",
    "        dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
    "        file_paths = sorted(glob.glob(dir_path))\n",
    "        machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
    "            [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
    "        return machine_id_list\n",
    "\n",
    "    def test_file_list_generator(target_dir, id_name, dir_name=\"test\", prefix_normal=\"normal\", prefix_anomaly=\"anomaly\", ext=\"wav\"):\n",
    "        \"\"\"\n",
    "        target_dir : str\n",
    "            base directory path of the dev_data or eval_data\n",
    "        id_name : str\n",
    "            id of wav file in <<test_dir_name>> directory\n",
    "        dir_name : str (default=\"test\")\n",
    "            directory containing test data\n",
    "        prefix_normal : str (default=\"normal\")\n",
    "            normal directory name\n",
    "        prefix_anomaly : str (default=\"anomaly\")\n",
    "            anomaly directory name\n",
    "        ext : str (default=\"wav\")\n",
    "            file extension of audio files\n",
    "\n",
    "        return :\n",
    "            if the mode is \"development\":\n",
    "                test_files : list [ str ]\n",
    "                    file list for test\n",
    "                test_labels : list [ boolean ]\n",
    "                    label info. list for test\n",
    "                    * normal/anomaly = 0/1\n",
    "            if the mode is \"evaluation\":\n",
    "                test_files : list [ str ]\n",
    "                    file list for test\n",
    "        \"\"\"\n",
    "        print(\"target_dir : {}\".format(target_dir + \"_\" + id_name))\n",
    "        normal_files = sorted(\n",
    "            glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
    "                                                                                 dir_name=dir_name,\n",
    "                                                                                 prefix_normal=prefix_normal,\n",
    "                                                                                 id_name=id_name,\n",
    "                                                                                 ext=ext)))\n",
    "        normal_labels = numpy.zeros(len(normal_files))\n",
    "        anomaly_files = sorted(\n",
    "            glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
    "                                                                                  dir_name=dir_name,\n",
    "                                                                                  prefix_anomaly=prefix_anomaly,\n",
    "                                                                                  id_name=id_name,\n",
    "                                                                                  ext=ext)))\n",
    "        anomaly_labels = numpy.ones(len(anomaly_files))\n",
    "        files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
    "        labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
    "        print(\"test_file  num : {num}\".format(num=len(files)))\n",
    "        if len(files) == 0:\n",
    "            print(\"Exception: no_wav_file!!\")\n",
    "        print(\"\\n========================================\")\n",
    "        return files, labels\n",
    "\n",
    "    def save_csv(save_file_path, save_data):\n",
    "        \"\"\"\n",
    "        Write csv data to specified path\n",
    "        \"\"\"\n",
    "        with open(save_file_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f, lineterminator='\\n')\n",
    "            writer.writerows(save_data)\n",
    "\n",
    "    dirs = select_dirs(dataset_path)\n",
    "    csv_lines = []\n",
    "    metrics_list = []\n",
    "    for idx, target_dir in enumerate(dirs):\n",
    "        print(\"\\n===========================\")\n",
    "        print(\"[{idx}/{total}] {dirname}\".format(dirname=target_dir, idx=idx + 1, total=len(dirs)))\n",
    "        machine_type = os.path.split(target_dir)[1]\n",
    "        model_file_path = \"{model}/model_{machine_type}.hdf5\".format(model=models_dir + '/model', machine_type=machine_type)\n",
    "\n",
    "        # load model file\n",
    "        print(\"============== MODEL LOAD ==============\")\n",
    "        if not os.path.exists(model_file_path):\n",
    "            print(\"{} model not found \".format(machine_type))\n",
    "            sys.exit(-1)\n",
    "\n",
    "        model = load_model(model_file_path)\n",
    "        model.summary()\n",
    "\n",
    "        # results by type\n",
    "        csv_lines.append([machine_type])\n",
    "        csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
    "        performance = []\n",
    "\n",
    "        machine_id_list = get_machine_id_list_for_test(target_dir)\n",
    "        print('Machine_id_list: ' + str(machine_id_list))\n",
    "\n",
    "        for id_str in machine_id_list:\n",
    "            # load test file\n",
    "            test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
    "            anomaly_score_list = []\n",
    "            print(\"\\n============== BEGIN TEST FOR A MACHINE ID ==============\")\n",
    "            y_scores = [0. for k in test_files]\n",
    "            for file_idx, file_path in enumerate(test_files):\n",
    "                try:\n",
    "                    data = file_to_vector_array(file_path, n_mels=feature_n_mels, frames=feature_frames, n_fft=feature_n_fft, hop_length=feature_hop_length, power=feature_power)\n",
    "                    errors = numpy.mean(numpy.square(data - model.predict(data)), axis=1)\n",
    "                    y_scores[file_idx] = numpy.mean(errors)\n",
    "                    anomaly_score_list.append([os.path.basename(file_path), y_scores[file_idx]])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(str(e))\n",
    "                    print(\"file broken!!: {}\".format(file_path))\n",
    "\n",
    "            # save anomaly score\n",
    "            if not os.path.exists(anomaly_dir):\n",
    "                os.makedirs(anomaly_dir)\n",
    "            anomaly_csv = os.path.join(anomaly_dir, 'anomaly_score_' + machine_type + \"_\" + id_str)\n",
    "            save_csv(save_file_path=anomaly_csv, save_data=anomaly_score_list)\n",
    "\n",
    "            if not os.path.exists(labels_dir):\n",
    "                os.makedirs(labels_dir)\n",
    "\n",
    "            # Save true labels and computed scores for metric generation\n",
    "            with open(f'{labels_dir}/y_labels.txt', 'w') as ft:\n",
    "                ft.write(str(list(y_true)))\n",
    "\n",
    "            with open(f'{labels_dir}/y_scores.txt', 'w') as fp:\n",
    "                fp.write(str(y_scores))\n",
    "\n",
    "            # append AUC and pAUC to lists\n",
    "            auc = metrics.roc_auc_score(y_true, y_scores)\n",
    "            p_auc = metrics.roc_auc_score(y_true, y_scores, max_fpr=max_fpr)\n",
    "            csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
    "            performance.append([auc, p_auc])\n",
    "            print(\"AUC : {}\".format(auc))\n",
    "            print(\"pAUC : {}\".format(p_auc))\n",
    "\n",
    "            metrics_list.append({\n",
    "                'name': machine_type + \"_\" + id_str + \"_AUC\",\n",
    "                'numberValue': str(auc),\n",
    "                'format': \"PERCENTAGE\",\n",
    "            })\n",
    "            metrics_list.append({\n",
    "                'name': machine_type + \"_\" + id_str + \"_pAUC\",\n",
    "                'numberValue': str(p_auc),\n",
    "                'format': \"PERCENTAGE\",\n",
    "            })\n",
    "\n",
    "            # append precision score\n",
    "            precision = metrics.average_precision_score(y_true, y_scores)\n",
    "            metrics_list.append({\n",
    "                'name': machine_type + \"_\" + id_str + \"_precision\",\n",
    "                'numberValue': str(precision),\n",
    "                'format': \"PERCENTAGE\",\n",
    "            })\n",
    "\n",
    "            print(\"\\n============ END OF TEST FOR A MACHINE ID ============\")\n",
    "\n",
    "        # calculate averages for AUCs and pAUCs\n",
    "        averaged_performance = numpy.mean(numpy.array(performance, dtype=float), axis=0)\n",
    "        csv_lines.append([\"Average\"] + list(averaged_performance))\n",
    "        csv_lines.append([])\n",
    "\n",
    "    # output results\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "    results_csv = os.path.join(results_dir, 'results.csv')\n",
    "    save_csv(save_file_path=results_csv, save_data=csv_lines)\n",
    "\n",
    "    with open(mlpipelinemetrics_path, 'w') as f:\n",
    "        json.dump(metrics_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Metrics and ROC Curve components**\n",
    "\n",
    "The following python functions will become a component later. \n",
    "This components get the reference from the file written by the test component and transforms it into visualizations the kubeflow ui can understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metrics(mlpipelinemetrics_path: comp.InputPath()) -> typing.NamedTuple('Outputs', [('mlpipeline_metrics', 'Metrics')]):\n",
    "    import json\n",
    "    with open(mlpipelinemetrics_path, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "\n",
    "    return [json.dumps({'metrics': metrics})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.components as comp\n",
    "import typing\n",
    "\n",
    "\n",
    "def roc_curve(labels_dir: comp.InputPath()) -> typing.NamedTuple('roc_curve', [('mlpipeline_ui_metadata', 'UI_metadata')]):\n",
    "    import json\n",
    "    from collections import namedtuple\n",
    "    from sklearn import metrics\n",
    "\n",
    "    # Load test labels and predicted labels\n",
    "    with open(f'{labels_dir}/y_labels.txt', 'r') as fl:\n",
    "        y_true = eval(fl.read())\n",
    "\n",
    "    with open(f'{labels_dir}/y_scores.txt', 'r') as fs:\n",
    "        y_scores = eval(fs.read())\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_scores)\n",
    "\n",
    "    csv_literal_roc_curve = \"\"\n",
    "    for i in range(len(fpr)):\n",
    "        csv_literal_roc_curve += \"{fpr},{tpr},{thresholds}\\n\".format(fpr=fpr[i], tpr=tpr[i], thresholds=thresholds[i])\n",
    "\n",
    "    kf_literal_roc_curve = {\n",
    "        'outputs' : [{\n",
    "            'type': 'roc',\n",
    "            'format': 'csv',\n",
    "            'schema': [\n",
    "                {'name': 'fpr', 'type': 'NUMBER'},\n",
    "                {'name': 'tpr', 'type': 'NUMBER'},\n",
    "                {'name': 'thresholds', 'type': 'NUMBER'},\n",
    "            ],\n",
    "            'storage': 'inline',\n",
    "            'source': csv_literal_roc_curve,\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    roc_curve_result = namedtuple('roc_curve', ['mlpipeline_ui_metadata'])\n",
    "    return roc_curve_result(json.dumps(kf_literal_roc_curve))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7 Create download, train and predict lightweight components.**\n",
    "\n",
    "Transform python funtions into components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages_to_install = ['pathlib', 'pyunpack', 'patool', 'keras==2.1.6', 'Keras-Applications==1.0.8', 'Keras-Preprocessing==1.1.0', 'numpy==1.16.0', 'PyYAML==5.1', 'scikit-learn==0.20.2', 'librosa==0.6.0', 'numba==0.48', 'audioread==2.1.5', 'setuptools==41.0.0', 'matplotlib']\n",
    "download_op = comp.func_to_container_op(download_dataset, base_image='tensorflow/tensorflow:latest-gpu-py3')\n",
    "train_op = comp.func_to_container_op(train, base_image='tensorflow/tensorflow:latest-gpu-py3', packages_to_install=packages_to_install)\n",
    "test_op = comp.func_to_container_op(test, base_image ='tensorflow/tensorflow:latest-gpu-py3', packages_to_install=packages_to_install)\n",
    "generate_metrics_op = comp.func_to_container_op(generate_metrics)\n",
    "roc_curve_op = comp.func_to_container_op(roc_curve, packages_to_install=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8 Define the pipeline: connections**\n",
    "\n",
    "Define all the connections of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "    name='anomalous_sound_detection_pipeline',\n",
    "    description='Pipeline for detecting anomalous sounds'\n",
    ")\n",
    "def task2_pipeline(\n",
    "    dataset_url: str = 'https://zenodo.org/record/3678171/files/dev_data_pump.zip',\n",
    "    max_fpr: float = 0.1,\n",
    "    feature_n_mels: int = 128,\n",
    "    feature_frames: int = 5,\n",
    "    feature_n_fft: int = 1024,\n",
    "    feature_hop_length: int = 512,\n",
    "    feature_power: float = 2.0,\n",
    "    fit_compile_optimizer: str = \"adam\",\n",
    "    fit_compile_loss: str = \"mean_squared_error\",\n",
    "    fit_epochs: int = 50,\n",
    "    fit_batch_size: int = 512,\n",
    "    fit_shuffle: bool = True,\n",
    "    fit_validation_split: float = 0.15,\n",
    "    fit_verbose: int = 1,\n",
    "):\n",
    "    result = download_op(dataset_url)\n",
    "    result.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    train_process = train_op(result.output, feature_frames, feature_hop_length, feature_n_fft, feature_n_mels, feature_power, fit_batch_size, fit_compile_loss, fit_compile_optimizer, fit_epochs, fit_shuffle, fit_validation_split, fit_verbose, max_fpr)\n",
    "    testing_process = test_op(result.output, feature_frames, feature_hop_length, feature_n_fft, feature_n_mels, feature_power, fit_batch_size, fit_compile_loss, fit_compile_optimizer, fit_epochs, fit_shuffle, fit_validation_split, fit_verbose, max_fpr, train_process.outputs['models_dir']).after(train_process)\n",
    "    generate_metrics_op(testing_process.outputs['mlpipelinemetrics'])\n",
    "    roc_curve_op(testing_process.outputs['labels_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8 Connect with the client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COOKIE = 'YOUR_COOKIE'\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "# Disable ssl verification\n",
    "from kfp_server_api.configuration import Configuration as Configuration\n",
    "if 'old_init' not in globals():\n",
    "    old_init = Configuration.__init__\n",
    "print('Monkeypatching')\n",
    "def new_init(self, *k, **kw):\n",
    "    old_init(self, *k, **kw)\n",
    "    self.verify_ssl = False\n",
    "Configuration.__init__ = new_init\n",
    "cookies = COOKIE\n",
    "client = kfp.Client(host='http://istio-ingressgateway.istio-system.svc/pipeline', namespace='admin', cookies=cookies)\n",
    "client.list_experiments(namespace=\"admin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9 Compile pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = task2_pipeline\n",
    "experiment_name = 'pump_experiment'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  '{}.zip'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 Define arguments and start executing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARGUMENTS DEFINITION\n",
    "arguments = {\n",
    "    \"dataset_url\" : \"https://zenodo.org/record/3678171/files/dev_data_pump.zip\",\n",
    "    \"max_fpr\" : 0.1,\n",
    "    \"feature_n_mels\" : 128,\n",
    "    \"feature_frames\" : 5,\n",
    "    \"feature_n_fft\" : 1024,\n",
    "    \"feature_hop_length\" : 512,\n",
    "    \"feature_power\" : 2.0,\n",
    "    \"fit_compile_optimizer\" : \"adam\",\n",
    "    \"fit_compile_loss\" : \"mean_squared_error\",\n",
    "    \"fit_epochs\" : 100,\n",
    "    \"fit_batch_size\" : 512,\n",
    "    \"fit_shuffle\" : True,\n",
    "    \"fit_validation_split\" : 0.15,\n",
    "    \"fit_verbose\" : 1\n",
    "}\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  namespace='admin',\n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}